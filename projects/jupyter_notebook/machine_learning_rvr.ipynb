{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to import all the libraries we will be using. In this tutorial we will be using Keras, Matplotlib, NumPy, time, requests, sys, json, and OpenCV 4.1 (cv2). For more info on how to download these libraries, refer to the documentation posted by the creators of the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import load_model\n",
    "import time\n",
    "import cv2\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we load the **MNIST Dataset**. This dataset contains thousands of pictures of handwritten digits for us to train and test our neural network on. X variables contain the images we will train on while the y variables contain the corresponding labels for the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to create a random seed for NumPy's random number generator. We can do this by using the current time using the time.time() function and casting the result to an integer. \n",
    "\n",
    "We can make sure that our data loaded correctly by printing out an example from our training set using Matplotlib's imshow() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13844dcc0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = int(time.time())\n",
    "np.random.seed(seed)\n",
    "plt.imshow(X_train[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we use the data to train our neural network, we have to \"clean up\" the data to make it easier for our neural network to make accurate predictions after it is trained. We can do this by inverting the color of the images and makeing them only black and white (color values of 255 or 0). The final data transformation is done using the `reshape()` function. This function turns the data into a 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for i in range(0,len(X_train)):\n",
    "    X_train[i] = cv2.bitwise_not(X_train[i])\n",
    "    X_train[i][X_train[i]<255] = 0\n",
    "\n",
    "for i in range(0, len(X_test)):\n",
    "    X_test[i]= cv2.bitwise_not(X_test[i])\n",
    "    X_test[i][X_test[i]<255] = 0\n",
    "\n",
    "\n",
    "plt.imshow(X_train[2])\n",
    "\n",
    "\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can normalize our data by dividing each data point by 255. Because the data is currently grayscale, the color values can range between 0-255. Normalizing the data gives us a smaller range of variability in our inputs/outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since computers don't automatically know the difference between number labels such as \"One\" or \"Seven\", we can use a **one hot encoding** of the values for each class by calling the `np_utils.to_categorical()` function. To learn more about how **one hot encoding** works, click [here](https://medium.com/@oraheem/one-hot-encoding-in-machine-learning-b2d344284d9e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to create the model itself. Our model will be a simple neural network with a single hidden layer that has 784 neurons. This layer will use [RELU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) as its activation function. The activation function that we will use on our output layer will be a [softmax](https://en.wikipedia.org/wiki/Softmax_function). This allows our outputs to be proababilities corresponding to which number the neural network is predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This cell should only be run if you have already saved a model. This allows the user to not re-train a model that has already been trained.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the cell where we fit the model to the data. The model is trained on the data for 10 epochs, and it is tested against the `X_test` dataset. Through this process, we can measure the accuracy of our model by its error. This cell may take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0725 16:28:57.509578 4561819072 deprecation_wrapper.py:119] From /Users/abajo01/.pyenv/versions/3.5.3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0725 16:28:57.556096 4561819072 deprecation_wrapper.py:119] From /Users/abajo01/.pyenv/versions/3.5.3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0725 16:28:57.572103 4561819072 deprecation_wrapper.py:119] From /Users/abajo01/.pyenv/versions/3.5.3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "W0725 16:28:57.618942 4561819072 deprecation_wrapper.py:119] From /Users/abajo01/.pyenv/versions/3.5.3/lib/python3.5/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0725 16:28:57.651184 4561819072 deprecation_wrapper.py:119] From /Users/abajo01/.pyenv/versions/3.5.3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0725 16:28:57.770878 4561819072 deprecation.py:323] From /Users/abajo01/.pyenv/versions/3.5.3/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0725 16:28:57.832034 4561819072 deprecation_wrapper.py:119] From /Users/abajo01/.pyenv/versions/3.5.3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.4933 - acc: 0.8526 - val_loss: 0.2966 - val_acc: 0.9144\n",
      "Epoch 2/10\n",
      " - 5s - loss: 0.2637 - acc: 0.9207 - val_loss: 0.2057 - val_acc: 0.9380\n",
      "Epoch 3/10\n",
      " - 5s - loss: 0.2005 - acc: 0.9402 - val_loss: 0.1707 - val_acc: 0.9519\n",
      "Epoch 4/10\n",
      " - 4s - loss: 0.1564 - acc: 0.9525 - val_loss: 0.1447 - val_acc: 0.9586\n",
      "Epoch 5/10\n",
      " - 4s - loss: 0.1313 - acc: 0.9608 - val_loss: 0.1226 - val_acc: 0.9626\n",
      "Epoch 6/10\n",
      " - 4s - loss: 0.1163 - acc: 0.9644 - val_loss: 0.1300 - val_acc: 0.9617\n",
      "Epoch 7/10\n",
      " - 4s - loss: 0.1020 - acc: 0.9691 - val_loss: 0.1136 - val_acc: 0.9674\n",
      "Epoch 8/10\n",
      " - 4s - loss: 0.0928 - acc: 0.9715 - val_loss: 0.1103 - val_acc: 0.9666\n",
      "Epoch 9/10\n",
      " - 4s - loss: 0.0844 - acc: 0.9738 - val_loss: 0.1061 - val_acc: 0.9676\n",
      "Epoch 10/10\n",
      " - 5s - loss: 0.0764 - acc: 0.9761 - val_loss: 0.1034 - val_acc: 0.9675\n",
      "Baseline Error: 3.25%\n"
     ]
    }
   ],
   "source": [
    "model = baseline_model()\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training our model, we can save it to a .h5 file so we can load it later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell sets up the payloads to send to the Node.js server that will be running on the Raspberry Pi. Since the Raspberry Pi is not powerful enough to train a neural network, we train and use the model on the PC/Mac, then send commands over to the pi based on the model's predictions.\n",
    "\n",
    "The `pi_ip` variable should be replaced with the IP address of your Raspberry Pi.\n",
    "\n",
    "**NOTE: You must have the Node.js server running on the Raspberry Pi before the next cell is run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_ip = ''\n",
    "lights_url = 'http://{}:2010/api/v1.0/io/setAllLedsWith32BitMask/1'.format(pi_ip)\n",
    "motors_url = 'http://{}:2010/api/v1.0/drive/rawMotors/2'.format(pi_ip)\n",
    "drive_url = 'http://{}:2010/api/v1.0/drive/driveWithHeading/2'.format(pi_ip)\n",
    "wake_url = 'http://{}:2010/api/v1.0/power/wake/1'.format(pi_ip)\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'accept': 'application/json'\n",
    "}\n",
    "\n",
    "\n",
    "payload0 = {\n",
    "        \"leftMode\" : 1,\n",
    "        \"rightMode\" : 2,\n",
    "        \"leftSpeed\" : 255,\n",
    "        \"rightSpeed\" : 255\n",
    "}\n",
    "\n",
    "payload1 = {\n",
    "        \"flags\": 0,\n",
    "        \"speed\": 128,\n",
    "        \"heading\": 0\n",
    "}\n",
    "\n",
    "payload2 = {\n",
    "        \"flags\": 0,\n",
    "        \"speed\": 128,\n",
    "        \"heading\": 90\n",
    "}\n",
    "\n",
    "payload3 = {\n",
    "        \"flags\": 0,\n",
    "        \"speed\": 128,\n",
    "        \"heading\": 180\n",
    "}\n",
    "\n",
    "payload4 = {\n",
    "        \"flags\": 0,\n",
    "        \"speed\": 128,\n",
    "        \"heading\": 270\n",
    "}\n",
    "\n",
    "payload5 = {\n",
    "        \"ledBrightnessValues\": [255, 0, 0,\n",
    "                                255, 0, 0,\n",
    "                                255, 0, 0,\n",
    "                                255, 0, 0,\n",
    "                                255, 0, 0,\n",
    "                                255, 0, 0,\n",
    "                                255, 0, 0,\n",
    "                                255, 0, 0,\n",
    "                                255, 0, 0,\n",
    "                                255, 0, 0],\n",
    "        \"ledGroup\": 1073741823\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "payload6 = {\n",
    "        \"ledBrightnessValues\": [0, 255, 0,\n",
    "                                0, 255, 0,\n",
    "                                0, 255, 0,\n",
    "                                0, 255, 0,\n",
    "                                0, 255, 0,\n",
    "                                0, 255, 0,\n",
    "                                0, 255, 0,\n",
    "                                0, 255, 0,\n",
    "                                0, 255, 0,\n",
    "                                0, 255, 0],\n",
    "        \"ledGroup\": 1073741823\n",
    "}\n",
    "\n",
    "payload7 = {\n",
    "        \"ledBrightnessValues\": [0, 0, 255,\n",
    "                                0, 0, 255,\n",
    "                                0, 0, 255,\n",
    "                                0, 0, 255,\n",
    "                                0, 0, 255,\n",
    "                                0, 0, 255,\n",
    "                                0, 0, 255,\n",
    "                                0, 0, 255,\n",
    "                                0, 0, 255,\n",
    "                                0, 0, 255],\n",
    "        \"ledGroup\": 1073741823\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "payload8 = {\n",
    "        \"ledBrightnessValues\": [255, 255, 0,\n",
    "                                255, 255, 0,\n",
    "                                255, 255, 0,\n",
    "                                255, 255, 0,\n",
    "                                255, 255, 0,\n",
    "                                255, 255, 0,\n",
    "                                255, 255, 0,\n",
    "                                255, 255, 0,\n",
    "                                255, 255, 0,\n",
    "                                255, 255, 0],\n",
    "        \"ledGroup\": 1073741823\n",
    "}\n",
    "\n",
    "payload9 = {\n",
    "        \"ledBrightnessValues\": [255, 255, 255,\n",
    "                                255, 255, 255,\n",
    "                                255, 255, 255,\n",
    "                                255, 255, 255,\n",
    "                                255, 255, 255,\n",
    "                                255, 255, 255,\n",
    "                                255, 255, 255,\n",
    "                                255, 255, 255,\n",
    "                                255, 255, 255,\n",
    "                                255, 255, 255],\n",
    "        \"ledGroup\": 1073741823\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final cell, we start a camera feed. Because we only want the most important details from the camera feed, there are multiple filters applied to the image. We also resize it in the same way that we resized the training data.\n",
    "When on the pop-up window, if the 'q' key is pressed, the neural network will output its prediciton of what is currently on the camera and send the corresponding command to RVR. If the 'e' key is pressed, the feed will stop and the program with end until you re-run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "requests.put(wake_url)\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 320)\n",
    "\n",
    "ret, frame = video_capture.read()\n",
    "print(frame.shape)\n",
    "ret,frame = video_capture.read()\n",
    "# Capture frame-by-frame\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    width = int(frame.shape[1] * 4.375 / 100)\n",
    "    height = int(frame.shape[0] * 6 / 100)\n",
    "    dim = (width, height)\n",
    "    \n",
    "    frame = cv2.resize(frame, dim, interpolation = cv2.INTER_AREA)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame = cv2.GaussianBlur(frame,(3,3),0)\n",
    "    frame[frame<=100] = 0\n",
    "    frame[frame>100] = 255\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2,2))\n",
    "    frame = cv2.morphologyEx(frame, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    pixels = frame.shape[0] * frame.shape[1]\n",
    "    reshaped = frame.reshape(pixels).astype('float32')\n",
    "    reshaped = reshaped /255\n",
    "    \n",
    "    cv2.imshow('Video', frame)\n",
    "    value = model.predict( np.array([reshaped,]))    \n",
    "    \n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        if np.amax(value) >= .5:\n",
    "            decision = np.where(value == np.amax(value))[1][0]\n",
    "            print(decision)\n",
    "            print(frame.shape)\n",
    "            if decision == 1:\n",
    "                response = requests.put(drive_url, headers=headers,data=json.dumps(payload1))\n",
    "            elif decision == 2:\n",
    "                response = requests.put(drive_url, headers=headers,data=json.dumps(payload2))\n",
    "            elif decision == 3:\n",
    "                response = requests.put(drive_url, headers=headers,data=json.dumps(payload3))\n",
    "            elif decision == 4:\n",
    "                response = requests.put(drive_url, headers=headers,data=json.dumps(payload4))\n",
    "            elif decision == 5:\n",
    "                response = requests.put(lights_url, headers=headers,data=json.dumps(payload5))\n",
    "            elif decision == 6:\n",
    "                response = requests.put(lights_url, headers=headers,data=json.dumps(payload6))\n",
    "            elif decision == 7:\n",
    "                response = requests.put(lights_url, headers=headers,data=json.dumps(payload7))\n",
    "            elif decision == 8:\n",
    "                response = requests.put(lights_url, headers=headers,data=json.dumps(payload8))\n",
    "            elif decision == 9:\n",
    "                response = requests.put(lights_url, headers=headers,data=json.dumps(payload9))\n",
    "            elif decision == 0:\n",
    "                response = requests.put(motors_url, headers=headers,data=json.dumps(payload0))\n",
    "            \n",
    "            plt.imshow(frame)\n",
    "        else:\n",
    "            print('no number found!!')\n",
    "            #print(value)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('e'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
